{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266502a-ad64-4971-b5be-2199dc2dfa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support,precision_recall_curve, roc_curve, auc,matthews_corrcoef \n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from evaluate import load\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a2c97-0b13-4f2a-bc6c-8aef5123f98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"esm2_t33_650M_UR50D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86c1de-e5ed-4371-a4dc-8774e5dc2430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 查看ESM模型的层\n",
    "# esm_model = AutoModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "# # 打印ESM模型的层次结构\n",
    "# print(esm_model)\n",
    "\n",
    "# # 打印ESM模型的所有参数名称\n",
    "# for name, param in esm_model.named_parameters():\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc538229-75a1-482f-86c4-0a272c785556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/total40.tsv', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def995d-1b79-4f97-9f3e-ba94e25d870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清除含有缺失值的行\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399f773-fe88-40db-90e6-187cd36cdf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过正则表达式找到每个类别的标签\n",
    "dna = df['Gene Ontology (GO)'].str.contains(\"DNA-binding\")\n",
    "rna = df['Gene Ontology (GO)'].str.contains(\"RNA-binding\")\n",
    "# non = df['Gene Ontology (GO)'].str.contains(\"Non-binding\")\n",
    "non = ~dna & ~rna  # 反例中不包括核酸结合蛋白的情况\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1705c-c790-484b-95b5-6ce0844e49ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_df = df[dna & ~rna & ~non]\n",
    "dna_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762a285-8c0e-4d8c-ba2f-a17e153abfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_df = df[rna & ~dna & ~non]\n",
    "rna_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be633b61-29cd-41f0-a735-612ae7778398",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_df = df[non & ~dna & ~rna]\n",
    "non_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326b707-58cc-4792-b661-303d1f64a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_sequences = non_df[\"Sequence\"].tolist()\n",
    "non_labels = [0 for protein in non_sequences]# 非核酸结合蛋白标签为0\n",
    "nucleic_sequences = df[dna | rna][\"Sequence\"].tolist()\n",
    "nucleic_labels = [1 for protein in nucleic_sequences]  # 核酸结合蛋白标签为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07babc66-3952-425c-ad76-416167513067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二阶段的标签分配\n",
    "dna_sequences = dna_df[\"Sequence\"].tolist()\n",
    "dna_labels = [0 for protein in dna_sequences] # DNA绑定蛋白标签为0\n",
    "rna_sequences = rna_df[\"Sequence\"].tolist()\n",
    "rna_labels = [1 for protein in rna_sequences]  # RNA绑定蛋白标签为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7d6cc7-4dfa-4a51-b3fb-7da8b57f3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并序列和标签\n",
    "sequences = non_sequences + nucleic_sequences  # 第一阶段的序列和标签\n",
    "labels = non_labels + nucleic_labels\n",
    "\n",
    "# 确认序列和标签数量一致\n",
    "assert len(sequences) == len(labels), \"序列和标签数量不匹配\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad10b8d-3a5a-4123-9652-be37c7ce030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先创建完整的数据集\n",
    "data = {\n",
    "    \"sequence\": sequences,  # 这个sequences应包含第一阶段的所有序列\n",
    "    \"label\": labels         # labels是对应的标签，用于第一阶段的分类\n",
    "}\n",
    "full_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d9704-3750-481a-a57f-eda8bd9d731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为第二阶段准备数据\n",
    "second_stage_data = {\n",
    "    \"sequence\": dna_sequences + rna_sequences,  # 合并DNA和RNA序列\n",
    "    \"label\": dna_labels + rna_labels            # 对应的标签为第二阶段的分类\n",
    "}\n",
    "second_stage_full_data = pd.DataFrame(second_stage_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7166c70-a72e-417b-9979-b839f45af6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac954c0a-ffca-4fa3-bf1a-67de2cda5099",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinSequenceDataset(Dataset):\n",
    "    \"\"\"蛋白质序列数据集\"\"\"\n",
    "    def __init__(self, sequences, labels, tokenizer, max_length=1000, augmentation_prob=0.0):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 数据增强只在训练时应用\n",
    "        if self.augmentation_prob > 0 and random.random() < self.augmentation_prob:\n",
    "            sequence = self.augment_sequence(sequence)\n",
    "\n",
    "        # 对序列进行编码，设置 max_length 为 1000\n",
    "        encoded_sequence = self.tokenizer(sequence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        \n",
    "        input_ids = encoded_sequence['input_ids'].squeeze(0)  # 移除批次维度\n",
    "        attention_mask = encoded_sequence['attention_mask'].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.float).unsqueeze(0)  # 将标签转换为 [1] 形状的张量\n",
    "        }\n",
    "\n",
    "    def augment_sequence(self, sequence):\n",
    "        \"\"\"对序列进行数据增强\"\"\"\n",
    "        seq_list = list(sequence)\n",
    "        seq_len = len(seq_list)\n",
    "        \n",
    "        # 随机选择一种增强方式\n",
    "        augmentation_choice = random.choice(['delete', 'swap', 'insert', 'replace'])\n",
    "        \n",
    "        if augmentation_choice == 'delete' and seq_len > 200:\n",
    "            # 随机删除一个氨基酸，仅在序列长度大于200时进行\n",
    "            del seq_list[random.randint(0, seq_len - 1)]\n",
    "        \n",
    "        elif augmentation_choice == 'swap' and seq_len > 1:\n",
    "            # 随机交换两个氨基酸\n",
    "            idx1, idx2 = random.sample(range(seq_len), 2)\n",
    "            seq_list[idx1], seq_list[idx2] = seq_list[idx2], seq_list[idx1]\n",
    "        \n",
    "        elif augmentation_choice == 'insert' and seq_len < 1000:\n",
    "            # 随机插入一个氨基酸，仅在序列长度小于1000时进行\n",
    "            amino_acid = random.choice(seq_list)\n",
    "            seq_list.insert(random.randint(0, seq_len), amino_acid)\n",
    "        \n",
    "        elif augmentation_choice == 'replace' and seq_len > 0:\n",
    "            # 随机替换一个氨基酸\n",
    "            idx = random.randint(0, seq_len - 1)\n",
    "            seq_list[idx] = random.choice(seq_list)\n",
    "        \n",
    "        return ''.join(seq_list)\n",
    "\n",
    "    def set_augmentation_prob(self, augmentation_prob):\n",
    "        \"\"\"设置数据增强的概率\"\"\"\n",
    "        self.augmentation_prob = augmentation_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54331511-de8d-44eb-84e5-3b0b8828cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集和数据加载器实例\n",
    "def create_data_loaders(train_data, val_data, test_data, batch_size=8, augmentation_prob=0):\n",
    "    train_dataset = ProteinSequenceDataset(train_data['sequence'].tolist(), train_data['label'].tolist(), tokenizer, augmentation_prob=augmentation_prob)\n",
    "    val_dataset = ProteinSequenceDataset(val_data['sequence'].tolist(), val_data['label'].tolist(), tokenizer, augmentation_prob=0)\n",
    "    test_dataset = ProteinSequenceDataset(test_data['sequence'].tolist(), test_data['label'].tolist(), tokenizer, augmentation_prob=0)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd32c0e-aa90-49b2-a16c-f37397c80690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pr_curve(labels, probs, stage, fold, save_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    绘制PR曲线并保存为图像文件\n",
    "    Args:\n",
    "        labels (list or array): 真实标签\n",
    "        probs (list or array): 预测概率值\n",
    "        stage (int): 当前阶段\n",
    "        fold (int): 当前折叠\n",
    "        save_dir (str): 保存图像文件的目录\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f'pr_curve_stage_{stage}_fold_{fold}.png')\n",
    "\n",
    "    precision, recall, _ = precision_recall_curve(labels, probs)\n",
    "    aupr = auc(recall, precision)  # 计算AUPR\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(recall, precision, color='#6976A3', linewidth=2, label=f'AUPR = {aupr:.4f}')\n",
    "    plt.plot([0, 1], [1, 0.5], color='gray', linestyle='--', linewidth=1)  # 添加对角参考线\n",
    "    plt.xlabel('Recall', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Precision', fontsize=16, fontweight='bold')\n",
    "    plt.title(f'Precision-Recall Curve for Stage {stage} Fold {fold}', fontsize=18, fontweight='bold')\n",
    "    plt.legend(loc='lower left', fontsize=15)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.ylim([0.475, 1.025])  # 设置y轴的显示范围\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(labels, probs, stage, fold, save_dir=\"results\"):\n",
    "    \"\"\"\n",
    "    绘制ROC曲线并保存为图像文件\n",
    "    Args:\n",
    "        labels (list or array): 真实标签\n",
    "        probs (list or array): 预测概率值\n",
    "        stage (int): 当前阶段\n",
    "        fold (int): 当前折叠\n",
    "        save_dir (str): 保存图像文件的目录\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f'roc_curve_stage_{stage}_fold_{fold}.png')\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(labels, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(fpr, tpr, color='#7F77CB', label=f'AUC = {roc_auc:.4f}', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1)  # 添加对角参考线\n",
    "    plt.xlabel('False Positive Rate', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontsize=16, fontweight='bold')\n",
    "    plt.title(f'ROC Curve for Stage {stage} Fold {fold}', fontsize=18, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=15)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ce0fb-9e7d-478e-b782-e0a072570565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, loss_function, scheduler, device, num_epochs, stage, fold, initial_augmentation_prob, patience=None):\n",
    "    model.to(device)  # 确保模型在正确的设备上 \n",
    "    # 检查是否使用 DataParallel 并调用 set_stage\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.set_stage(stage)\n",
    "    else:\n",
    "        model.set_stage(stage)\n",
    "  # 设置模型阶段\n",
    "    best_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    learning_rates = []  # 记录每个epoch的学习率\n",
    "    train_metrics_history = []  # 记录每个epoch的训练精度、召回率和F1值\n",
    "    val_metrics_history = []  # 记录每个epoch的验证精度、召回率和F1值\n",
    "\n",
    "    # 动态调整数据增强概率\n",
    "    augmentation_prob = initial_augmentation_prob\n",
    "    \n",
    "    for epoch in range(num_epochs):         \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "         # 设置当前epoch的数据增强概率\n",
    "        dataloaders['train'].dataset.set_augmentation_prob(augmentation_prob)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            for batch in dataloaders[phase]:\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs, attention_mask)           \n",
    "                    loss = loss_function(outputs, labels)\n",
    "\n",
    "                    # 使用sigmoid获取概率值，并设置阈值0.5进行分类\n",
    "                    probs = torch.sigmoid(outputs)\n",
    "                    preds = (probs >= 0.5).float()\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            # 记录损失和精度\n",
    "            if phase == 'train':\n",
    "                train_loss_history.append(epoch_loss)\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "                train_metrics_history.append({'precision': precision, 'recall': recall, 'f1': f1})\n",
    "            else:\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "                val_metrics_history.append({'precision': precision, 'recall': recall, 'f1': f1})\n",
    "\n",
    "            # 打印阶段日志\n",
    "            print(f\"{phase.capitalize()} Stage {stage} Epoch {epoch+1}: Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_accuracy:\n",
    "                    best_accuracy = epoch_acc\n",
    "                    best_model_state = model.state_dict()  # 保存最优模型状态\n",
    "                    epochs_no_improve = 0\n",
    "                    print(f\"the best_model_state with accuracy {best_accuracy:.4f}\")\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if patience and epochs_no_improve >= patience:\n",
    "                        print(f\"Early stopping after {patience} epochs without improvement\")\n",
    "                        return model, best_model_state\n",
    "\n",
    "        # 更新学习率\n",
    "        if phase == 'val':\n",
    "            scheduler.step(epoch_loss)\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "\n",
    "        # 动态调整数据增强概率，例如随着 epoch 增加逐渐减少\n",
    "        augmentation_prob *= 0.90  # 每个 epoch 后减少 10%\n",
    "\n",
    "    print(f\"Training complete with best validation accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    return model, best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72acc80-5d0e-48eb-a34d-b7c7f26fdf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, target_names, stage, fold, save_dir=\"results\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, f'confusion_matrix_stage_{stage}_fold_{fold}.png')\n",
    "\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            annot[i, j] = f'{cm[i, j]}\\n({cm_percent[i, j]*100:.2f}%)'\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=annot, fmt='', cmap=\"GnBu\", xticklabels=target_names, yticklabels=target_names, cbar=True, annot_kws={\"size\": 14, \"weight\": \"bold\"})\n",
    "    \n",
    "    plt.xlabel('Predicted Labels', fontsize=18, fontweight='bold')\n",
    "    plt.ylabel('True Labels', fontsize=18, fontweight='bold')\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.title('Confusion Matrix for Stage ' + str(stage) + f' Fold {fold}', fontsize=17, fontweight='bold')\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05196da2-f914-4682-9750-5259fa5c70ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, stage, fold):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(inputs, attention_mask)\n",
    "            \n",
    "            # Use sigmoid to get probability values\n",
    "            probs = torch.sigmoid(outputs)\n",
    "\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Convert probabilities to binary predictions with a threshold of 0.5\n",
    "    all_preds = (np.array(all_probs) >= 0.5).astype(float)\n",
    "\n",
    "    # Set class names\n",
    "    if stage == 1:\n",
    "        target_names = ['Non-Nucleic Acid-Binding', 'Nucleic Acid-Binding']\n",
    "    elif stage == 2:\n",
    "        target_names = ['DNA-Binding', 'RNA-Binding']\n",
    "    else:\n",
    "        raise ValueError(\"Stage must be 1 or 2\")\n",
    "\n",
    "    # Print detailed classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=target_names, output_dict=True)\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names))\n",
    "\n",
    "    # Compute and print confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Visualize confusion matrix\n",
    "    plot_confusion_matrix(cm, target_names, stage, fold, save_dir=\"results\")\n",
    "    \n",
    "    # Extract performance metrics from classification report\n",
    "    accuracy = report['accuracy']\n",
    "    precision = report['macro avg']['precision']\n",
    "    recall = report['macro avg']['recall']\n",
    "    f1 = report['macro avg']['f1-score']\n",
    "    \n",
    "    # Manually calculate main performance metrics to verify\n",
    "    accuracy_manual = accuracy_score(all_labels, all_preds)\n",
    "    precision_manual, recall_manual, f1_manual, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # Calculate sensitivity (SN) and specificity (SP)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = recall_manual  # Recall represents sensitivity\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    \n",
    "    # Print manually calculated results for comparison\n",
    "    print(f\"Manual Calculations - Accuracy: {accuracy_manual:.4f}, Precision: {precision_manual:.4f}, Recall (Sensitivity): {recall_manual:.4f}, F1: {f1_manual:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}, MCC: {mcc:.4f}\")\n",
    "    \n",
    "    # Calculate and plot PR curve and ROC curve\n",
    "    plot_pr_curve(all_labels, all_probs, stage, fold, save_dir=\"results\")\n",
    "    plot_roc_curve(all_labels, all_probs, stage, fold, save_dir=\"results\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1, sensitivity, specificity, mcc, all_labels, all_probs, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28c7942-cbc8-43bc-b830-2808570dfd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPathProteinClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiPathProteinClassifier, self).__init__()\n",
    "        self.esm2 = AutoModel.from_pretrained(model_checkpoint)\n",
    "        \n",
    "        # 路径1:  Transformer + CNN     \n",
    "        self.transformer = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=1280, nhead=8, batch_first=True,dropout=0.6),num_layers=1)  # 根据ESM-2输出调整\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=1280, out_channels=512, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.AdaptiveAvgPool1d(output_size=500),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(in_channels=512, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.AdaptiveAvgPool1d(output_size=200),\n",
    "            nn.Dropout(0.2),\n",
    "            \n",
    "            nn.Conv1d(in_channels=128, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.AdaptiveAvgPool1d(output_size=20),\n",
    "            nn.Dropout(0.2),\n",
    "        )\n",
    "        \n",
    "        # 路径2: BiLSTM + Attention\n",
    "        self.bilstm = nn.LSTM(input_size=1280, hidden_size=256, num_layers=1, batch_first=True, bidirectional=True)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=512, num_heads=4, batch_first=True)  # 使用BiLSTM的输出维度     \n",
    "        self.attention_pool = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6)\n",
    "        )\n",
    "        # 对Attention的输出进行池化       \n",
    "        \n",
    "        # 特征融合\n",
    "        self.feature_fusion = nn.Sequential(\n",
    "            nn.Linear(896, 256),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Dropout(0.6),            \n",
    "        )\n",
    "        \n",
    "        # 第一阶段和第二阶段的分类器\n",
    "        self.classifier_stage1 = nn.Linear(256, 1)  # 核酸结合与非核酸结合\n",
    "        self.classifier_stage2 = nn.Linear(256, 1)  # DNA与RNA\n",
    "        \n",
    "        # 当前训练阶段\n",
    "        self.current_stage = 1\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # ESM-2 输出\n",
    "        shared_output = self.esm2(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "\n",
    "        # 路径1: Transformer + CNN\n",
    "        x1 = self.transformer(shared_output)  # Transformer处理\n",
    "        x1 = x1.permute(0, 2, 1)  # 为CNN调整维度\n",
    "        x1 = self.cnn(x1).view(x1.size(0), -1)  # Flatten CNN输出\n",
    "        #print(\"CNN output shape:\", x1.shape)  # 查看CNN输出尺寸\n",
    "\n",
    "        # 路径2: BiLSTM + Attention\n",
    "        x2, _ = self.bilstm(shared_output)\n",
    "        attention_output, _ = self.attention(x2, x2, x2)\n",
    "        x2 = attention_output.mean(dim=1)  # 平均池化\n",
    "        x2 = self.attention_pool(x2)\n",
    "        #print(\"BiLSTM output shape:\", x2.shape)  # 查看BiLSTM输出尺寸\n",
    "        \n",
    "        # 特征融合\n",
    "        features = torch.cat([x1, x2], dim=1)\n",
    "        #print(\"Concatenated feature1 shape:\", features.shape)\n",
    "        features = self.feature_fusion(features)\n",
    "        #print(\"Concatenated feature2 shape:\", features.shape)\n",
    "\n",
    "        # 根据当前阶段使用相应的分类器\n",
    "        if self.current_stage == 1:\n",
    "            return self.classifier_stage1(features)\n",
    "        else:\n",
    "            return self.classifier_stage2(features)\n",
    "        \n",
    "    def set_stage(self, stage):\n",
    "        self.current_stage = stage\n",
    "\n",
    "\n",
    "    def freeze_layers(self):\n",
    "        for param in self.esm2.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_last_layers(self, num_layers_to_unfreeze=10):\n",
    "        # 获取最后 num_layers_to_unfreeze 个编码器层的名字\n",
    "        layer_names = [f'encoder.layer.{i}' for i in range(32 - num_layers_to_unfreeze, 33)]\n",
    "        for name, param in self.esm2.named_parameters():\n",
    "            if any(name.startswith(layer_name) for layer_name in layer_names):\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcba4c0a-d08a-4777-bd4a-f459f943b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_pr_roc_curves(fold_count, stage, save_path_pr=None, save_path_roc=None):\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    for fold in range(1, fold_count + 1):\n",
    "        # 假设 load_intermediate_results 返回标签、概率和其他信息\n",
    "        fold_labels, fold_probs, _ = load_intermediate_results(stage, fold)\n",
    "        print(f\"Fold {fold} - Labels: {len(fold_labels)}, Probs: {len(fold_probs)}\")  # 调试信息\n",
    "        all_labels.extend(fold_labels)\n",
    "        all_probs.extend(fold_probs)\n",
    "\n",
    "    # 确保所有标签和概率值的长度一致\n",
    "    print(f\"Total Labels: {len(all_labels)}, Total Probs: {len(all_probs)}\")  # 调试信息\n",
    "    if len(all_labels) != len(all_probs):\n",
    "        raise ValueError(\"Mismatch between total labels and probs length\")\n",
    "\n",
    "    # 计算 PR 曲线\n",
    "    precision, recall, _ = precision_recall_curve(all_labels, all_probs)\n",
    "    aupr = auc(recall, precision)  # 计算AUPR\n",
    "    # 计算 ROC 曲线\n",
    "    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # 绘制 PR 曲线\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(recall, precision, color='#6976A3', linewidth=2, label=f'AUPR = {aupr:.4f}')\n",
    "    plt.plot([0, 1], [1, 0.5], color='gray', linestyle='--', linewidth=1)  # 添加对角参考线\n",
    "    plt.xlabel('Recall', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('Precision', fontsize=16, fontweight='bold')\n",
    "    plt.title(f'Average Precision-Recall Curve for Stage {stage}', fontsize=18, fontweight='bold')\n",
    "    plt.legend(loc='lower left', fontsize=15)\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.ylim([0.475, 1.025])  # 设置y轴的显示范围\n",
    "    if save_path_pr:\n",
    "        plt.savefig(save_path_pr, dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # 绘制 ROC 曲线\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.plot(fpr, tpr, color='#7F77CB', label=f'AUC = {roc_auc:.4f}', linewidth=2)\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=1)  # 添加对角参考线\n",
    "    plt.xlabel('False Positive Rate', fontsize=16, fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontsize=16, fontweight='bold')\n",
    "    plt.title(f'Average ROC Curve for Stage {stage}', fontsize=18, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=15)  \n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    if save_path_roc:\n",
    "        plt.savefig(save_path_roc, dpi=600, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fefa6c-9833-4bbc-a8c9-8f6edb498d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_confusion_matrix(fold_count, class_names, stage, save_path=None):\n",
    "    confusion_matrices = []\n",
    "\n",
    "    for fold in range(1, fold_count+1):\n",
    "        _, _, cm = load_intermediate_results(stage, fold)\n",
    "        confusion_matrices.append(cm)\n",
    "\n",
    "    avg_cm = np.mean(confusion_matrices, axis=0)\n",
    "    avg_cm_percent = avg_cm.astype('float') / avg_cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    annot = np.empty_like(avg_cm).astype(str)\n",
    "    for i in range(avg_cm.shape[0]):\n",
    "        for j in range(avg_cm.shape[1]):\n",
    "            annot[i, j] = f'{int(avg_cm[i, j])}\\n({avg_cm_percent[i, j]*100:.2f}%)'\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(avg_cm, annot=annot, fmt='', cmap=\"GnBu\", xticklabels=class_names, yticklabels=class_names, cbar=True, annot_kws={\"size\": 14, \"weight\": \"bold\"})\n",
    "\n",
    "    plt.xlabel('Predicted Labels', fontsize=18, fontweight='bold')\n",
    "    plt.ylabel('True Labels', fontsize=18, fontweight='bold')\n",
    "    plt.xticks(fontsize=15)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.title(f'Average Confusion Matrix for Stage {stage}', fontsize=17, fontweight='bold')\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb2d2fc-ba5d-412e-8f5e-d94bbde0cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_intermediate_results(stage, fold, labels, probs, cm):\n",
    "    if len(labels) != len(probs):\n",
    "        raise ValueError(\"Labels and probabilities length mismatch\")\n",
    "    with open(f\"fold/labels_stage_{stage}_fold_{fold}.pkl\", 'wb') as f:\n",
    "        pickle.dump(labels, f)\n",
    "    with open(f\"fold/probs_stage_{stage}_fold_{fold}.pkl\", 'wb') as f:\n",
    "        pickle.dump(probs, f)\n",
    "    with open(f\"fold/cm_stage_{stage}_fold_{fold}.pkl\", 'wb') as f:\n",
    "        pickle.dump(cm, f)\n",
    "\n",
    "def load_intermediate_results(stage, fold):\n",
    "    with open(f\"fold/labels_stage_{stage}_fold_{fold}.pkl\", 'rb') as f:\n",
    "        labels = pickle.load(f)\n",
    "    with open(f\"fold/probs_stage_{stage}_fold_{fold}.pkl\", 'rb') as f:\n",
    "        probs = pickle.load(f)\n",
    "    with open(f\"fold/cm_stage_{stage}_fold_{fold}.pkl\", 'rb') as f:\n",
    "        cm = pickle.load(f)\n",
    "    return labels, probs, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd6a5d-b4e2-4dc4-838b-a72e754fb0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr_pretrained=5e-6, lr_custom=5e-5, weight_decay_pretrained=1e-2, weight_decay_custom=3e-1):\n",
    "    pretrained_decay_params = []\n",
    "    pretrained_no_decay_params = []\n",
    "    custom_params = []\n",
    "\n",
    "    param_iter = model.module.named_parameters() if isinstance(model, nn.DataParallel) else model.named_parameters()\n",
    "\n",
    "    for name, param in param_iter:\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if \"esm2\" in name:\n",
    "            if \"bias\" in name or \"LayerNorm.weight\" in name:\n",
    "                pretrained_no_decay_params.append(param)\n",
    "            else:\n",
    "                pretrained_decay_params.append(param)\n",
    "        else:\n",
    "            custom_params.append(param)\n",
    "\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': list(filter(lambda p: p.requires_grad, pretrained_decay_params)), 'lr': lr_pretrained, 'weight_decay': weight_decay_pretrained},\n",
    "        {'params': list(filter(lambda p: p.requires_grad, pretrained_no_decay_params)), 'lr': lr_pretrained, 'weight_decay': 0.0},\n",
    "        {'params': list(filter(lambda p: p.requires_grad, custom_params)), 'lr': lr_custom, 'weight_decay': weight_decay_custom}\n",
    "    ])\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12168d-5459-4a9b-b1c8-1705dcd5fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查CUDA是否可用，并使用它\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81926d3a-25a0-4492-badd-d88714e4059a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stratified_k_fold_cross_validation_stage1(full_data, tokenizer, k=5, num_epochs=50, initial_augmentation_prob=0.5):\n",
    "    # 首先将数据集划分为训练集和独立的测试集\n",
    "    train_val_data_stage1, test_data_stage1 = train_test_split(full_data, test_size=0.2, random_state=42, stratify=full_data['label'])\n",
    "    # 保存 test_data_stage1 到 CSV 文件\n",
    "    test_data_stage1.to_csv('Data/test_data_stage1.csv', index=False)\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for train_index, val_index in skf.split(train_val_data_stage1['sequence'], train_val_data_stage1['label']):\n",
    "        fold += 1\n",
    "        print(f\"Fold {fold}/{k}\")\n",
    "\n",
    "        # 使用StratifiedKFold的索引划分训练+验证集\n",
    "        train_data_stage1 = train_val_data_stage1.iloc[train_index]\n",
    "        val_data_stage1 = train_val_data_stage1.iloc[val_index]\n",
    "\n",
    "        train_loader_stage1, val_loader_stage1, test_loader_stage1 = create_data_loaders(train_data_stage1, val_data_stage1, test_data_stage1, augmentation_prob=initial_augmentation_prob)\n",
    "\n",
    "        # 每个折叠重新初始化模型\n",
    "        model = MultiPathProteinClassifier()\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        model = model.to(device)\n",
    "\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.freeze_layers()\n",
    "        else:\n",
    "            model.freeze_layers()\n",
    "        \n",
    "        # 创建优化器\n",
    "        optimizer = get_optimizer(model)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.6, patience=2, min_lr=1e-6)\n",
    "\n",
    "        \n",
    "        # 第一阶段训练\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.set_stage(1)\n",
    "        else:\n",
    "            model.set_stage(1)\n",
    "\n",
    "        # 计算第一阶段的pos_weight\n",
    "        num_positive1 = train_data_stage1['label'].sum()\n",
    "        num_negative1 = len(train_data_stage1) - num_positive1\n",
    "        if num_positive1 < num_negative1:\n",
    "            pos_weight1 = torch.tensor([num_negative1 / num_positive1], dtype=torch.float32).to(device)\n",
    "        else:\n",
    "            pos_weight1 = torch.tensor([num_positive1 / num_negative1], dtype=torch.float32).to(device)\n",
    "\n",
    "        # 设置第一阶段的损失函数\n",
    "        loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight1)\n",
    "\n",
    "        print(\"Training Stage 1:\")\n",
    "        torch.cuda.empty_cache()  # 清理未使用的显存\n",
    "        model, best_model_state_fold = train_model(model, {'train': train_loader_stage1, 'val': val_loader_stage1}, optimizer, loss_function, scheduler, device, num_epochs, stage=1, fold=fold, initial_augmentation_prob=initial_augmentation_prob, patience=4)\n",
    "        \n",
    "        # 评估第一阶段\n",
    "        print(\"Evaluating Stage 1:\")\n",
    "        accuracy, precision, recall, f1, sensitivity, specificity, mcc, fold_labels, fold_probs, cm_stage1 = evaluate_model(model, test_loader_stage1, device, stage=1, fold=fold)\n",
    "        print(f\"Stage 1 Evaluation - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall (Sensitivity): {recall:.4f}, F1: {f1:.4f}, Specificity: {specificity:.4f}, MCC: {mcc:.4f}\")\n",
    "\n",
    "        save_intermediate_results(stage=1, fold=fold, labels=fold_labels, probs=fold_probs, cm=cm_stage1)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_state = best_model_state_fold\n",
    "\n",
    "        del model, optimizer, scheduler, train_loader_stage1, val_loader_stage1, test_loader_stage1\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "# 执行第一阶段的分层k折交叉验证\n",
    "best_model_state_stage1 = stratified_k_fold_cross_validation_stage1(full_data, tokenizer, k=5, num_epochs=50, initial_augmentation_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7e132-0eda-413a-bdde-d1b74ff6879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载并合并所有中间结果\n",
    "all_labels_stage1 = []\n",
    "all_probs_stage1 = []\n",
    "confusion_matrices_stage1 = []\n",
    "k = 5\n",
    "for fold in range(1, k+1):\n",
    "    fold_labels, fold_probs, cm = load_intermediate_results(stage=1, fold=fold)\n",
    "    all_labels_stage1.extend(fold_labels)\n",
    "    all_probs_stage1.extend(fold_probs)\n",
    "    confusion_matrices_stage1.append(cm)\n",
    "\n",
    "# 绘制第一阶段的平均PR曲线和ROC曲线\n",
    "plot_average_pr_roc_curves(fold_count=k, stage=1, save_path_pr=\"results/avg_pr_curve_stage_1.png\", save_path_roc=\"results/avg_roc_curve_stage_1.png\")\n",
    "\n",
    "# 绘制第一阶段的平均混淆矩阵\n",
    "class_names_stage1 = ['Non-Nucleic Acid-Binding', 'Nucleic Acid-Binding']\n",
    "plot_average_confusion_matrix(fold_count=k, class_names=class_names_stage1, stage=1, save_path=\"results/avg_confusion_matrix_stage_1.png\")\n",
    "\n",
    "# 保存第一阶段的最佳模型状态\n",
    "torch.save(best_model_state_stage1, \"Model/best_model_stage1.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44985998-f585-4ad1-ad99-66d129b97721",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(model, best_model_state_stage1):\n",
    "    \"\"\"\n",
    "    Load weights into the model, handling the differences between single GPU and multi-GPU training states.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model into which the weights will be loaded.\n",
    "        best_model_state_stage1 (dict): The state dictionary of the best model from stage 1.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If there is a mismatch between the keys in the state dictionary and the model.\n",
    "    \"\"\"\n",
    "    model_state_dict = model.state_dict()\n",
    "    new_state_dict = {}\n",
    "\n",
    "    # Iterate over the items in the state dictionary from stage 1\n",
    "    for k, v in best_model_state_stage1.items():\n",
    "        # Check if the current key in the model's state dictionary starts with 'module.'\n",
    "        if k.startswith(\"module.\") and not next(iter(model_state_dict.keys())).startswith(\"module.\"):\n",
    "            new_state_dict[k[7:]] = v\n",
    "        elif not k.startswith(\"module.\") and next(iter(model_state_dict.keys())).startswith(\"module.\"):\n",
    "            new_state_dict[\"module.\" + k] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "    try:\n",
    "        # Load the newly constructed state dictionary into the model\n",
    "        model.load_state_dict(new_state_dict)\n",
    "        print(\"Model weights loaded successfully.\")\n",
    "    except KeyError as e:\n",
    "        print(\"Error in loading model weights: \", e)\n",
    "        raise\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed13043b-dd58-423e-9bbb-9e18a0d89f2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stratified_k_fold_cross_validation_stage2(second_stage_full_data, tokenizer, best_model_state_stage1, k=5, num_epochs=50, initial_augmentation_prob=0.5):\n",
    "     # 首先将数据集划分为训练+验证集和独立的测试集\n",
    "    train_val_data_stage2, test_data_stage2 = train_test_split(second_stage_full_data, test_size=0.2, random_state=42, stratify=second_stage_full_data['label'])\n",
    "    # 保存 test_data_stage2 到 CSV 文件\n",
    "    test_data_stage2.to_csv('Data/test_data_stage2.csv', index=False)\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for train_index, val_index in skf.split(train_val_data_stage2['sequence'], train_val_data_stage2['label']):\n",
    "        fold += 1\n",
    "        print(f\"Fold {fold}/{k}\")\n",
    "\n",
    "        # 使用StratifiedKFold的索引划分训练+验证集\n",
    "        train_data_stage2 = train_val_data_stage2.iloc[train_index]\n",
    "        val_data_stage2 = train_val_data_stage2.iloc[val_index]\n",
    "\n",
    "        # 创建数据加载器\n",
    "        train_loader_stage2, val_loader_stage2, test_loader_stage2 = create_data_loaders(train_data_stage2, val_data_stage2, test_data_stage2, augmentation_prob=initial_augmentation_prob)\n",
    "\n",
    "        # 每个折叠重新初始化模型\n",
    "        model = MultiPathProteinClassifier()\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        model = model.to(device)\n",
    "\n",
    "\n",
    "        # 加载第一阶段的最佳模型状态\n",
    "        model = load_model_weights(model, best_model_state_stage1)\n",
    "        \n",
    "       \n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.freeze_layers()\n",
    "        else:\n",
    "            model.freeze_layers()\n",
    "            \n",
    "        # 创建优化器\n",
    "        optimizer = get_optimizer(model)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.6, patience=2, min_lr=1e-6)\n",
    "\n",
    "        \n",
    "        # 第二阶段训练\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.set_stage(2)\n",
    "        else:\n",
    "            model.set_stage(2)\n",
    "\n",
    "        # 计算第二阶段的pos_weight\n",
    "        num_positive2 = train_data_stage2['label'].sum()\n",
    "        num_negative2 = len(train_data_stage2) - num_positive2\n",
    "        if num_positive2 < num_negative2:\n",
    "            pos_weight2 = torch.tensor([num_negative2 / num_positive2], dtype=torch.float32).to(device)\n",
    "        else:\n",
    "            pos_weight2 = torch.tensor([num_positive2 / num_negative2], dtype=torch.float32).to(device)\n",
    "\n",
    "        # 设置第二阶段的损失函数\n",
    "        loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight2)\n",
    "\n",
    "        print(\"Training Stage 2:\")\n",
    "        torch.cuda.empty_cache()  # 清理未使用的显存\n",
    "        model, best_model_state_fold = train_model(model, {'train': train_loader_stage2, 'val': val_loader_stage2}, optimizer, loss_function, scheduler, device, num_epochs, stage=2, fold=fold, initial_augmentation_prob=initial_augmentation_prob, patience=4)\n",
    "\n",
    "        print(\"Evaluating Stage 2:\")\n",
    "        accuracy, precision, recall, f1, sensitivity, specificity, mcc, fold_labels, fold_probs, cm_stage2 = evaluate_model(model, test_loader_stage2, device, stage=2, fold=fold)\n",
    "        print(f\"Stage 2 Evaluation - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall (Sensitivity): {recall:.4f}, F1: {f1:.4f}, Specificity: {specificity:.4f}, MCC: {mcc:.4f}\")\n",
    "\n",
    "\n",
    "        save_intermediate_results(stage=2, fold=fold, labels=fold_labels, probs=fold_probs, cm=cm_stage2)\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_state = best_model_state_fold\n",
    "\n",
    "        del model, optimizer, scheduler, train_loader_stage2, val_loader_stage2, test_loader_stage2\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "# 执行第二阶段的分层k折交叉验证\n",
    "best_model_state_stage2 = stratified_k_fold_cross_validation_stage2(second_stage_full_data, tokenizer, best_model_state_stage1, k=5, num_epochs=50, initial_augmentation_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed0085-7a29-4d90-b302-a7ce91dd74e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载并合并所有中间结果\n",
    "all_labels_stage2 = []\n",
    "all_probs_stage2 = []\n",
    "confusion_matrices_stage2 = []\n",
    "for fold in range(1, k+1):\n",
    "    fold_labels, fold_probs, cm = load_intermediate_results(stage=2, fold=fold)\n",
    "    all_labels_stage2.extend(fold_labels)\n",
    "    all_probs_stage2.extend(fold_probs)\n",
    "    confusion_matrices_stage2.append(cm)\n",
    "\n",
    "# 绘制第二阶段的平均PR曲线和ROC曲线\n",
    "plot_average_pr_roc_curves(fold_count=k, stage=2, save_path_pr=\"results/avg_pr_curve_stage_2.png\", save_path_roc=\"results/avg_roc_curve_stage_2.png\")\n",
    "\n",
    "# 绘制第二阶段的平均混淆矩阵\n",
    "class_names_stage2 = ['DNA-Binding', 'RNA-Binding']\n",
    "plot_average_confusion_matrix(fold_count=k, class_names=class_names_stage2, stage=2, save_path=\"results/avg_confusion_matrix_stage_2.png\")\n",
    "\n",
    "# 保存第二阶段的最佳模型状态\n",
    "torch.save(best_model_state_stage2, \"Model/best_model_stage2.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955c6e3-2cec-4bae-96ae-a434c7552dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e8b0d-c03d-46f6-87e6-1d3821c00b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建保存图像的文件夹\n",
    "if not os.path.exists('scatter_plots'):\n",
    "    os.makedirs('scatter_plots')\n",
    "\n",
    "def plot_length_vs_probability_non_binding(sequences, labels, probs, title, filename):\n",
    "    \"\"\"\n",
    "    绘制Non-Nucleic Acid-Binding类别的散点图，正确预测集中在1附近并保存图像.\n",
    "    \n",
    "    参数:\n",
    "    - sequences: 蛋白质序列列表.\n",
    "    - labels: 真实标签列表.\n",
    "    - probs: 预测概率列表.\n",
    "    - title: 图表标题，表示当前类别.\n",
    "    - filename: 保存图像的文件名.\n",
    "    \"\"\"\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    probs = 1 - np.array(probs)  # 反转概率，使得正确预测集中在1附近\n",
    "    \n",
    "    predictions = (probs >= 0.5).astype(float)\n",
    "    correct_indices = np.where(predictions == labels)[0]\n",
    "    incorrect_indices = np.where(predictions != labels)[0]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.scatter(\n",
    "        np.array(lengths)[incorrect_indices], \n",
    "        probs[incorrect_indices], \n",
    "        c='#8E84C2', label='Correct', alpha=0.6\n",
    "    )\n",
    "    plt.scatter(\n",
    "        np.array(lengths)[correct_indices], \n",
    "        probs[correct_indices], \n",
    "        c='#AAAFB6', label='Incorrect', alpha=0.6\n",
    "    )\n",
    "    \n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--', linewidth=1)\n",
    "    plt.xlabel('Sequence Length', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right', fontsize=11,markerscale=1.5)\n",
    "    plt.savefig(filename, dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_length_vs_probability_binding(sequences, labels, probs, title, filename):\n",
    "    \"\"\"\n",
    "    绘制Nucleic Acid-Binding类别的散点图，正确预测集中在1附近并保存图像.\n",
    "    \n",
    "    参数:\n",
    "    - sequences: 蛋白质序列列表.\n",
    "    - labels: 真实标签列表.\n",
    "    - probs: 预测概率列表.\n",
    "    - title: 图表标题，表示当前类别.\n",
    "    - filename: 保存图像的文件名.\n",
    "    \"\"\"\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    predictions = (np.array(probs) >= 0.5).astype(float)\n",
    "    \n",
    "    correct_indices = np.where(predictions == labels)[0]\n",
    "    incorrect_indices = np.where(predictions != labels)[0]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.scatter(\n",
    "        np.array(lengths)[correct_indices], \n",
    "        np.array(probs)[correct_indices], \n",
    "        c='#F84B34', label='Correct', alpha=0.6\n",
    "    )\n",
    "    plt.scatter(\n",
    "        np.array(lengths)[incorrect_indices], \n",
    "        np.array(probs)[incorrect_indices], \n",
    "        c='#AAAFB6', label='Incorrect', alpha=0.6\n",
    "    )\n",
    "    \n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--', linewidth=1)\n",
    "    plt.xlabel('Sequence Length', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right', fontsize=11,markerscale=1.5)\n",
    "    plt.savefig(filename, dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_folds_results_non_binding(folds_data, stage, category_name, label):\n",
    "    \"\"\"\n",
    "    针对每个fold生成Non-Nucleic Acid-Binding类别的散点图并保存图像.\n",
    "    \n",
    "    参数:\n",
    "    - folds_data: 每个fold的测试集数据，包含序列、标签和预测概率.\n",
    "    - stage: 当前阶段（1或2）.\n",
    "    - category_name: 当前类别名称.\n",
    "    - label: 当前类别标签.\n",
    "    \"\"\"\n",
    "    for fold, (sequences, labels, probs) in enumerate(folds_data, 1):\n",
    "        indices = [i for i, l in enumerate(labels) if l == label]\n",
    "        category_sequences = [sequences[i] for i in indices]\n",
    "        category_labels = [labels[i] for i in indices]\n",
    "        category_probs = [probs[i] for i in indices]\n",
    "        \n",
    "        filename = f\"scatter_plots/{category_name.replace(' ', '_')}_stage_{stage}_fold_{fold}.png\"\n",
    "        \n",
    "        plot_length_vs_probability_non_binding(\n",
    "            sequences=category_sequences,\n",
    "            labels=category_labels,\n",
    "            probs=category_probs,\n",
    "            title=f'Stage {stage} Fold {fold}: {category_name}',\n",
    "            filename=filename\n",
    "        )\n",
    "\n",
    "def plot_all_folds_results_binding(folds_data, stage, category_name, label):\n",
    "    \"\"\"\n",
    "    针对每个fold生成Nucleic Acid-Binding类别的散点图并保存图像.\n",
    "    \n",
    "    参数:\n",
    "    - folds_data: 每个fold的测试集数据，包含序列、标签和预测概率.\n",
    "    - stage: 当前阶段（1或2）.\n",
    "    - category_name: 当前类别名称.\n",
    "    - label: 当前类别标签.\n",
    "    \"\"\"\n",
    "    for fold, (sequences, labels, probs) in enumerate(folds_data, 1):\n",
    "        indices = [i for i, l in enumerate(labels) if l == label]\n",
    "        category_sequences = [sequences[i] for i in indices]\n",
    "        category_labels = [labels[i] for i in indices]\n",
    "        category_probs = [probs[i] for i in indices]\n",
    "        \n",
    "        filename = f\"scatter_plots/{category_name.replace(' ', '_')}_stage_{stage}_fold_{fold}.png\"\n",
    "        \n",
    "        plot_length_vs_probability_binding(\n",
    "            sequences=category_sequences,\n",
    "            labels=category_labels,\n",
    "            probs=category_probs,\n",
    "            title=f'Stage {stage} Fold {fold}: {category_name}',\n",
    "            filename=filename\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed914255-7328-4681-8f9b-5389acef3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载第一阶段的测试集数据\n",
    "test_data_stage1 = pd.read_csv('Data/test_data_stage1.csv')\n",
    "test_sequences_stage1 = test_data_stage1['sequence'].tolist()\n",
    "test_labels_stage1 = test_data_stage1['label'].tolist()\n",
    "\n",
    "# 获取每个fold的预测概率\n",
    "num_folds = 5\n",
    "folds_data_stage1 = [(test_sequences_stage1, *load_intermediate_results(stage=1, fold=i)[:2]) for i in range(1, num_folds + 1)]\n",
    "\n",
    "# 第一阶段的类别\n",
    "stage1_categories = {\n",
    "    'Non-Nucleic Acid-Binding': 0,\n",
    "    'Nucleic Acid-Binding': 1\n",
    "}\n",
    "\n",
    "# 绘制第一阶段的所有fold的结果\n",
    "for category_name, label in stage1_categories.items():\n",
    "    if category_name == 'Non-Nucleic Acid-Binding':\n",
    "        plot_all_folds_results_non_binding(folds_data_stage1, stage=1, category_name=category_name, label=label)\n",
    "    else:\n",
    "        plot_all_folds_results_binding(folds_data_stage1, stage=1, category_name=category_name, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678b8e9-01ed-432d-93c7-483536c064ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_length_vs_probability_dna_binding(sequences, labels, probs, title, filename):\n",
    "    \"\"\"\n",
    "    绘制DNA-Binding类别的散点图，正确预测集中在1附近并保存图像.\n",
    "    \n",
    "    参数:\n",
    "    - sequences: 蛋白质序列列表.\n",
    "    - labels: 真实标签列表.\n",
    "    - probs: 预测概率列表.\n",
    "    - title: 图表标题，表示当前类别.\n",
    "    - filename: 保存图像的文件名.\n",
    "    \"\"\"\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    probs = 1 - np.array(probs)  # 反转概率，使得正确预测集中在1附近\n",
    "\n",
    "    predictions = (probs >= 0.5).astype(float)\n",
    "    correct_indices = np.where(predictions == labels)[0]\n",
    "    incorrect_indices = np.where(predictions != labels)[0]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.scatter(\n",
    "        np.array(lengths)[incorrect_indices], \n",
    "        probs[incorrect_indices], \n",
    "        c='#FF972F', label='Correct', alpha=0.6\n",
    "    )\n",
    "    plt.scatter(\n",
    "        np.array(lengths)[correct_indices], \n",
    "        probs[correct_indices], \n",
    "        c='#AAAFB6', label='Incorrect', alpha=0.6\n",
    "    )\n",
    "    \n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--', linewidth=1)\n",
    "    plt.xlabel('Sequence Length', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right', fontsize=11,markerscale=1.5)\n",
    "    plt.savefig(filename, dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_length_vs_probability_rna_binding(sequences, labels, probs, title, filename):\n",
    "    \"\"\"\n",
    "    绘制RNA-Binding类别的散点图，正确预测集中在1附近并保存图像.\n",
    "    \n",
    "    参数:\n",
    "    - sequences: 蛋白质序列列表.\n",
    "    - labels: 真实标签列表.\n",
    "    - probs: 预测概率列表.\n",
    "    - title: 图表标题，表示当前类别.\n",
    "    - filename: 保存图像的文件名.\n",
    "    \"\"\"\n",
    "    lengths = [len(seq) for seq in sequences]\n",
    "    predictions = (np.array(probs) >= 0.5).astype(float)\n",
    "    \n",
    "    correct_indices = np.where(predictions == labels)[0]\n",
    "    incorrect_indices = np.where(predictions != labels)[0]\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.scatter(\n",
    "        np.array(lengths)[correct_indices], \n",
    "        np.array(probs)[correct_indices], \n",
    "        c='#5BB9B0', label='Correct', alpha=0.6\n",
    "    )\n",
    "    plt.scatter(\n",
    "        np.array(lengths)[incorrect_indices], \n",
    "        np.array(probs)[incorrect_indices], \n",
    "        c='#AAAFB6', label='Incorrect', alpha=0.6\n",
    "    )\n",
    "    \n",
    "    plt.axhline(y=0.5, color='gray', linestyle='--', linewidth=1)\n",
    "    plt.xlabel('Sequence Length', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Predicted Probability', fontsize=12, fontweight='bold')\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right', fontsize=11,markerscale=1.5)\n",
    "    plt.savefig(filename, dpi=600, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_folds_results_dna_binding(folds_data, stage, category_name, label):\n",
    "    \"\"\"\n",
    "    针对每个fold生成DNA-Binding类别的散点图并保存图像.\n",
    "    \n",
    "    参数:\n",
    "    - folds_data: 每个fold的测试集数据，包含序列、标签和预测概率.\n",
    "    - stage: 当前阶段（2）.\n",
    "    - category_name: 当前类别名称.\n",
    "    - label: 当前类别标签.\n",
    "    \"\"\"\n",
    "    for fold, (sequences, labels, probs) in enumerate(folds_data, 1):\n",
    "        indices = [i for i, l in enumerate(labels) if l == label]\n",
    "        category_sequences = [sequences[i] for i in indices]\n",
    "        category_labels = [labels[i] for i in indices]\n",
    "        category_probs = [probs[i] for i in indices]\n",
    "        \n",
    "        filename = f\"scatter_plots/{category_name.replace(' ', '_')}_stage_{stage}_fold_{fold}.png\"\n",
    "        \n",
    "        plot_length_vs_probability_dna_binding(\n",
    "            sequences=category_sequences,\n",
    "            labels=category_labels,\n",
    "            probs=category_probs,\n",
    "            title=f'Stage {stage} Fold {fold}: {category_name}',\n",
    "            filename=filename\n",
    "        )\n",
    "\n",
    "def plot_all_folds_results_rna_binding(folds_data, stage, category_name, label):\n",
    "    \"\"\"\n",
    "    针对每个fold生成RNA-Binding类别的散点图并保存图像.\n",
    "    \n",
    "    参数:\n",
    "    - folds_data: 每个fold的测试集数据，包含序列、标签和预测概率.\n",
    "    - stage: 当前阶段（2）.\n",
    "    - category_name: 当前类别名称.\n",
    "    - label: 当前类别标签.\n",
    "    \"\"\"\n",
    "    for fold, (sequences, labels, probs) in enumerate(folds_data, 1):\n",
    "        indices = [i for i, l in enumerate(labels) if l == label]\n",
    "        category_sequences = [sequences[i] for i in indices]\n",
    "        category_labels = [labels[i] for i in indices]\n",
    "        category_probs = [probs[i] for i in indices]\n",
    "        \n",
    "        filename = f\"scatter_plots/{category_name.replace(' ', '_')}_stage_{stage}_fold_{fold}.png\"\n",
    "        \n",
    "        plot_length_vs_probability_rna_binding(\n",
    "            sequences=category_sequences,\n",
    "            labels=category_labels,\n",
    "            probs=category_probs,\n",
    "            title=f'Stage {stage} Fold {fold}: {category_name}',\n",
    "            filename=filename\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0976297e-dbcf-4eee-9e56-3e8d9e3efaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载第二阶段的测试集数据\n",
    "test_data_stage2 = pd.read_csv('Data/test_data_stage2.csv')\n",
    "test_sequences_stage2 = test_data_stage2['sequence'].tolist()\n",
    "test_labels_stage2 = test_data_stage2['label'].tolist()\n",
    "\n",
    "# 获取每个fold的预测概率\n",
    "num_folds = 5\n",
    "folds_data_stage2 = [(test_sequences_stage2, *load_intermediate_results(stage=2, fold=i)[:2]) for i in range(1, num_folds + 1)]\n",
    "\n",
    "# 第二阶段的类别\n",
    "stage2_categories = {\n",
    "    'DNA-Binding': 0,\n",
    "    'RNA-Binding': 1\n",
    "}\n",
    "\n",
    "# 绘制第二阶段的所有fold的结果\n",
    "for category_name, label in stage2_categories.items():\n",
    "    if category_name == 'DNA-Binding':\n",
    "        plot_all_folds_results_dna_binding(folds_data_stage2, stage=2, category_name=category_name, label=label)\n",
    "    else:\n",
    "        plot_all_folds_results_rna_binding(folds_data_stage2, stage=2, category_name=category_name, label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b292a8-fb2d-44ff-afe3-4720e4817a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "mytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
