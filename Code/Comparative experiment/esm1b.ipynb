{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266502a-ad64-4971-b5be-2199dc2dfa68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.016373Z",
     "start_time": "2024-07-11T07:40:21.310647Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoModel\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support,precision_recall_curve, roc_curve, auc,matthews_corrcoef \n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from evaluate import load\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a2c97-0b13-4f2a-bc6c-8aef5123f98d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.032088Z",
     "start_time": "2024-07-11T07:40:24.018369Z"
    }
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"esm1b_t33_650M_UR50S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed0faee-9ee5-4348-9fe3-0072938e2d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载新的ESM模型并命名为 esm1b\n",
    "esm1b = AutoModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 打印ESM模型的层次结构\n",
    "print(esm1b)\n",
    "\n",
    "# 打印ESM模型的所有参数名称\n",
    "for name, param in esm1b.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc538229-75a1-482f-86c4-0a272c785556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.047576Z",
     "start_time": "2024-07-11T07:40:24.033262Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/total40.tsv', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def995d-1b79-4f97-9f3e-ba94e25d870b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.062859Z",
     "start_time": "2024-07-11T07:40:24.049600Z"
    }
   },
   "outputs": [],
   "source": [
    "# 清除含有缺失值的行\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399f773-fe88-40db-90e6-187cd36cdf51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.078326Z",
     "start_time": "2024-07-11T07:40:24.064363Z"
    }
   },
   "outputs": [],
   "source": [
    "# 通过正则表达式找到每个类别的标签\n",
    "dna = df['Gene Ontology (GO)'].str.contains(\"DNA-binding\")\n",
    "rna = df['Gene Ontology (GO)'].str.contains(\"RNA-binding\")\n",
    "# non = df['Gene Ontology (GO)'].str.contains(\"Non-binding\")\n",
    "non = ~dna & ~rna  # 反例中不包括核酸结合蛋白的情况\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe1705c-c790-484b-95b5-6ce0844e49ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.093996Z",
     "start_time": "2024-07-11T07:40:24.079328Z"
    }
   },
   "outputs": [],
   "source": [
    "dna_df = df[dna & ~rna & ~non]\n",
    "dna_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762a285-8c0e-4d8c-ba2f-a17e153abfba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.108962Z",
     "start_time": "2024-07-11T07:40:24.095000Z"
    }
   },
   "outputs": [],
   "source": [
    "rna_df = df[rna & ~dna & ~non]\n",
    "rna_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be633b61-29cd-41f0-a735-612ae7778398",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.124921Z",
     "start_time": "2024-07-11T07:40:24.110952Z"
    }
   },
   "outputs": [],
   "source": [
    "non_df = df[non & ~dna & ~rna]\n",
    "non_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3326b707-58cc-4792-b661-303d1f64a78e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.140874Z",
     "start_time": "2024-07-11T07:40:24.125917Z"
    }
   },
   "outputs": [],
   "source": [
    "non_sequences = non_df[\"Sequence\"].tolist()\n",
    "non_labels = [0 for protein in non_sequences]# 非核酸结合蛋白标签为0\n",
    "nucleic_sequences = df[dna | rna][\"Sequence\"].tolist()\n",
    "nucleic_labels = [1 for protein in nucleic_sequences]  # 核酸结合蛋白标签为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07babc66-3952-425c-ad76-416167513067",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.156829Z",
     "start_time": "2024-07-11T07:40:24.143863Z"
    }
   },
   "outputs": [],
   "source": [
    "# 第二阶段的标签分配\n",
    "dna_sequences = dna_df[\"Sequence\"].tolist()\n",
    "dna_labels = [0 for protein in dna_sequences] # DNA绑定蛋白标签为0\n",
    "rna_sequences = rna_df[\"Sequence\"].tolist()\n",
    "rna_labels = [1 for protein in rna_sequences]  # RNA绑定蛋白标签为1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7d6cc7-4dfa-4a51-b3fb-7da8b57f3fe3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.172916Z",
     "start_time": "2024-07-11T07:40:24.158825Z"
    }
   },
   "outputs": [],
   "source": [
    "# 合并序列和标签\n",
    "sequences = non_sequences + nucleic_sequences  # 第一阶段的序列和标签\n",
    "labels = non_labels + nucleic_labels\n",
    "\n",
    "# 确认序列和标签数量一致\n",
    "assert len(sequences) == len(labels), \"序列和标签数量不匹配\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad10b8d-3a5a-4123-9652-be37c7ce030d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.188753Z",
     "start_time": "2024-07-11T07:40:24.174913Z"
    }
   },
   "outputs": [],
   "source": [
    "# 首先创建完整的数据集\n",
    "data = {\n",
    "    \"sequence\": sequences,  # 这个sequences应包含第一阶段的所有序列\n",
    "    \"label\": labels         # labels是对应的标签，用于第一阶段的分类\n",
    "}\n",
    "full_data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d9704-3750-481a-a57f-eda8bd9d731b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.204825Z",
     "start_time": "2024-07-11T07:40:24.190751Z"
    }
   },
   "outputs": [],
   "source": [
    "# 为第二阶段准备数据\n",
    "second_stage_data = {\n",
    "    \"sequence\": dna_sequences + rna_sequences,  # 合并DNA和RNA序列\n",
    "    \"label\": dna_labels + rna_labels            # 对应的标签为第二阶段的分类\n",
    "}\n",
    "second_stage_full_data = pd.DataFrame(second_stage_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7166c70-a72e-417b-9979-b839f45af6ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.220701Z",
     "start_time": "2024-07-11T07:40:24.205808Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac954c0a-ffca-4fa3-bf1a-67de2cda5099",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.236246Z",
     "start_time": "2024-07-11T07:40:24.222626Z"
    }
   },
   "outputs": [],
   "source": [
    "class ProteinSequenceDataset(Dataset):\n",
    "    \"\"\"蛋白质序列数据集\"\"\"\n",
    "    def __init__(self, sequences, labels, tokenizer, max_length=1000, augmentation_prob=0.0):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augmentation_prob = augmentation_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 数据增强只在训练时应用\n",
    "        if self.augmentation_prob > 0 and random.random() < self.augmentation_prob:\n",
    "            sequence = self.augment_sequence(sequence)\n",
    "\n",
    "        # 对序列进行编码，设置 max_length 为 1000\n",
    "        encoded_sequence = self.tokenizer(sequence, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        \n",
    "        input_ids = encoded_sequence['input_ids'].squeeze(0)  # 移除批次维度\n",
    "        attention_mask = encoded_sequence['attention_mask'].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.float).unsqueeze(0)  # 将标签转换为 [1] 形状的张量\n",
    "        }\n",
    "\n",
    "    def augment_sequence(self, sequence):\n",
    "        \"\"\"对序列进行数据增强\"\"\"\n",
    "        seq_list = list(sequence)\n",
    "        seq_len = len(seq_list)\n",
    "        \n",
    "        # 随机选择一种增强方式\n",
    "        augmentation_choice = random.choice(['delete', 'swap', 'insert', 'replace'])\n",
    "        \n",
    "        if augmentation_choice == 'delete' and seq_len > 200:\n",
    "            # 随机删除一个氨基酸，仅在序列长度大于200时进行\n",
    "            del seq_list[random.randint(0, seq_len - 1)]\n",
    "        \n",
    "        elif augmentation_choice == 'swap' and seq_len > 1:\n",
    "            # 随机交换两个氨基酸\n",
    "            idx1, idx2 = random.sample(range(seq_len), 2)\n",
    "            seq_list[idx1], seq_list[idx2] = seq_list[idx2], seq_list[idx1]\n",
    "        \n",
    "        elif augmentation_choice == 'insert' and seq_len < 1000:\n",
    "            # 随机插入一个氨基酸，仅在序列长度小于1000时进行\n",
    "            amino_acid = random.choice(seq_list)\n",
    "            seq_list.insert(random.randint(0, seq_len), amino_acid)\n",
    "        \n",
    "        elif augmentation_choice == 'replace' and seq_len > 0:\n",
    "            # 随机替换一个氨基酸\n",
    "            idx = random.randint(0, seq_len - 1)\n",
    "            seq_list[idx] = random.choice(seq_list)\n",
    "        \n",
    "        return ''.join(seq_list)\n",
    "\n",
    "    def set_augmentation_prob(self, augmentation_prob):\n",
    "        \"\"\"设置数据增强的概率\"\"\"\n",
    "        self.augmentation_prob = augmentation_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54331511-de8d-44eb-84e5-3b0b8828cac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.251774Z",
     "start_time": "2024-07-11T07:40:24.237748Z"
    }
   },
   "outputs": [],
   "source": [
    "# 创建数据集和数据加载器实例\n",
    "def create_data_loaders(train_data, val_data, test_data, batch_size=8, augmentation_prob=0):\n",
    "    train_dataset = ProteinSequenceDataset(train_data['sequence'].tolist(), train_data['label'].tolist(), tokenizer, augmentation_prob=augmentation_prob)\n",
    "    val_dataset = ProteinSequenceDataset(val_data['sequence'].tolist(), val_data['label'].tolist(), tokenizer, augmentation_prob=0)\n",
    "    test_dataset = ProteinSequenceDataset(test_data['sequence'].tolist(), test_data['label'].tolist(), tokenizer, augmentation_prob=0)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ce0fb-9e7d-478e-b782-e0a072570565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.359665Z",
     "start_time": "2024-07-11T07:40:24.344699Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, loss_function, scheduler, device, num_epochs, stage, fold, initial_augmentation_prob, patience=None):\n",
    "    model.to(device)  # 确保模型在正确的设备上 \n",
    "    # 检查是否使用 DataParallel 并调用 set_stage\n",
    "    if isinstance(model, nn.DataParallel):\n",
    "        model.module.set_stage(stage)\n",
    "    else:\n",
    "        model.set_stage(stage)\n",
    "  # 设置模型阶段\n",
    "    best_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    learning_rates = []  # 记录每个epoch的学习率\n",
    "    train_metrics_history = []  # 记录每个epoch的训练精度、召回率和F1值\n",
    "    val_metrics_history = []  # 记录每个epoch的验证精度、召回率和F1值\n",
    "\n",
    "    # 动态调整数据增强概率\n",
    "    augmentation_prob = initial_augmentation_prob\n",
    "    \n",
    "    for epoch in range(num_epochs):         \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 20)\n",
    "\n",
    "         # 设置当前epoch的数据增强概率\n",
    "        dataloaders['train'].dataset.set_augmentation_prob(augmentation_prob)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            for batch in dataloaders[phase]:\n",
    "                inputs = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs, attention_mask)           \n",
    "                    loss = loss_function(outputs, labels)\n",
    "\n",
    "                    # 使用sigmoid获取概率值，并设置阈值0.5进行分类\n",
    "                    probs = torch.sigmoid(outputs)\n",
    "                    preds = (probs >= 0.5).float()\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            # 记录损失和精度\n",
    "            if phase == 'train':\n",
    "                train_loss_history.append(epoch_loss)\n",
    "                train_acc_history.append(epoch_acc)\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "                train_metrics_history.append({'precision': precision, 'recall': recall, 'f1': f1})\n",
    "            else:\n",
    "                val_loss_history.append(epoch_loss)\n",
    "                val_acc_history.append(epoch_acc)\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "                val_metrics_history.append({'precision': precision, 'recall': recall, 'f1': f1})\n",
    "\n",
    "            # 打印阶段日志\n",
    "            print(f\"{phase.capitalize()} Stage {stage} Epoch {epoch+1}: Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_accuracy:\n",
    "                    best_accuracy = epoch_acc\n",
    "                    best_model_state = model.state_dict()  # 保存最优模型状态\n",
    "                    epochs_no_improve = 0\n",
    "                    print(f\"the best_model_state with accuracy {best_accuracy:.4f}\")\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "                    if patience and epochs_no_improve >= patience:\n",
    "                        print(f\"Early stopping after {patience} epochs without improvement\")\n",
    "                        return model, best_model_state\n",
    "\n",
    "        # 更新学习率\n",
    "        if phase == 'val':\n",
    "            scheduler.step(epoch_loss)\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "\n",
    "        # 动态调整数据增强概率，例如随着 epoch 增加逐渐减少\n",
    "        augmentation_prob *= 0.90  # 每个 epoch 后减少 10%\n",
    "\n",
    "    print(f\"Training complete with best validation accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    return model, best_model_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05196da2-f914-4682-9750-5259fa5c70ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.390575Z",
     "start_time": "2024-07-11T07:40:24.376333Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device, stage, fold):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in dataloader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            outputs = model(inputs, attention_mask)\n",
    "            \n",
    "            # Use sigmoid to get probability values\n",
    "            probs = torch.sigmoid(outputs)\n",
    "\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Convert probabilities to binary predictions with a threshold of 0.5\n",
    "    all_preds = (np.array(all_probs) >= 0.5).astype(float)\n",
    "\n",
    "    # Set class names\n",
    "    if stage == 1:\n",
    "        target_names = ['Non-Nucleic Acid-Binding', 'Nucleic Acid-Binding']\n",
    "    elif stage == 2:\n",
    "        target_names = ['DNA-Binding', 'RNA-Binding']\n",
    "    else:\n",
    "        raise ValueError(\"Stage must be 1 or 2\")\n",
    "\n",
    "    # Print detailed classification report\n",
    "    report = classification_report(all_labels, all_preds, target_names=target_names, output_dict=True)\n",
    "    print(classification_report(all_labels, all_preds, target_names=target_names))\n",
    "\n",
    "    # Compute and print confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Extract performance metrics from classification report\n",
    "    accuracy = report['accuracy']\n",
    "    precision = report['macro avg']['precision']\n",
    "    recall = report['macro avg']['recall']\n",
    "    f1 = report['macro avg']['f1-score']\n",
    "    \n",
    "    # Manually calculate main performance metrics to verify\n",
    "    accuracy_manual = accuracy_score(all_labels, all_preds)\n",
    "    precision_manual, recall_manual, f1_manual, _ = precision_recall_fscore_support(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    # Calculate sensitivity (SN) and specificity (SP)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = recall_manual  # Recall represents sensitivity\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Calculate MCC\n",
    "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
    "    \n",
    "    # Print manually calculated results for comparison\n",
    "    print(f\"Manual Calculations - Accuracy: {accuracy_manual:.4f}, Precision: {precision_manual:.4f}, Recall (Sensitivity): {recall_manual:.4f}, F1: {f1_manual:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}, MCC: {mcc:.4f}\")\n",
    "    \n",
    "    return accuracy, precision, recall, f1, sensitivity, specificity, mcc, all_labels, all_probs, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1c8cf-b4bd-485f-8b05-399c9cd07051",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiPathProteinClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiPathProteinClassifier, self).__init__()\n",
    "        self.esm1b = AutoModel.from_pretrained(model_checkpoint)\n",
    "        \n",
    "        # 直接使用ESM-1b的输出进行分类\n",
    "        self.classifier_stage1 = nn.Linear(1280, 1)  # 核酸结合与非核酸结合\n",
    "        self.classifier_stage2 = nn.Linear(1280, 1)  # DNA与RNA\n",
    "        \n",
    "        # 当前训练阶段\n",
    "        self.current_stage = 1\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # ESM-1b 输出\n",
    "        shared_output = self.esm1b(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        \n",
    "        # 对ESM-1b输出取平均池化\n",
    "        features = shared_output.mean(dim=1)\n",
    "        \n",
    "        # 根据当前阶段使用相应的分类器\n",
    "        if self.current_stage == 1:\n",
    "            return self.classifier_stage1(features)\n",
    "        else:\n",
    "            return self.classifier_stage2(features)\n",
    "        \n",
    "    def set_stage(self, stage):\n",
    "        self.current_stage = stage\n",
    "\n",
    "    # def freeze_layers(self, layers_to_freeze):\n",
    "    #     for name, param in self.esm1b.named_parameters():\n",
    "    #         if any(layer in name for layer in layers_to_freeze):\n",
    "    #             param.requires_grad = False\n",
    "    \n",
    "    def freeze_layers(self):\n",
    "        for param in self.esm1b.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_layers(self, layers_to_unfreeze):\n",
    "        for name, param in self.esm1b.named_parameters():\n",
    "            if any(layer in name for layer in layers_to_unfreeze):\n",
    "                param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fd6a5d-b4e2-4dc4-838b-a72e754fb0d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.467795Z",
     "start_time": "2024-07-11T07:40:24.453761Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_optimizer(model, lr_pretrained=5e-6, lr_custom=5e-5, weight_decay_pretrained=1e-2, weight_decay_custom=3e-1):\n",
    "    pretrained_decay_params = []\n",
    "    pretrained_no_decay_params = []\n",
    "    custom_params = []\n",
    "\n",
    "    param_iter = model.module.named_parameters() if isinstance(model, nn.DataParallel) else model.named_parameters()\n",
    "\n",
    "    for name, param in param_iter:\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if \"esm1b\" in name:\n",
    "            if \"bias\" in name or \"LayerNorm.weight\" in name:\n",
    "                pretrained_no_decay_params.append(param)\n",
    "            else:\n",
    "                pretrained_decay_params.append(param)\n",
    "        else:\n",
    "            custom_params.append(param)\n",
    "\n",
    "    optimizer = optim.AdamW([\n",
    "        {'params': list(filter(lambda p: p.requires_grad, pretrained_decay_params)), 'lr': lr_pretrained, 'weight_decay': weight_decay_pretrained},\n",
    "        {'params': list(filter(lambda p: p.requires_grad, pretrained_no_decay_params)), 'lr': lr_pretrained, 'weight_decay': 0.0},\n",
    "        {'params': list(filter(lambda p: p.requires_grad, custom_params)), 'lr': lr_custom, 'weight_decay': weight_decay_custom}\n",
    "    ])\n",
    "\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12168d-5459-4a9b-b1c8-1705dcd5fc07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T07:40:24.499098Z",
     "start_time": "2024-07-11T07:40:24.484621Z"
    }
   },
   "outputs": [],
   "source": [
    "# 检查CUDA是否可用，并使用它\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67355c-d07f-413b-807d-13f9ccb2715a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-11T07:40:25.009Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stratified_k_fold_cross_validation_stage1(full_data, tokenizer, k=5, num_epochs=50, initial_augmentation_prob=0.5):\n",
    "    # 首先将数据集划分为训练集和独立的测试集\n",
    "    train_val_data_stage1, test_data_stage1 = train_test_split(full_data, test_size=0.2, random_state=42, stratify=full_data['label'])\n",
    "    # 保存 test_data_stage1 到 CSV 文件\n",
    "    test_data_stage1.to_csv('Data/test_data_stage1.csv', index=False)\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for train_index, val_index in skf.split(train_val_data_stage1['sequence'], train_val_data_stage1['label']):\n",
    "        fold += 1\n",
    "        print(f\"Fold {fold}/{k}\")\n",
    "\n",
    "        # 使用StratifiedKFold的索引划分训练+验证集\n",
    "        train_data_stage1 = train_val_data_stage1.iloc[train_index]\n",
    "        val_data_stage1 = train_val_data_stage1.iloc[val_index]\n",
    "\n",
    "        train_loader_stage1, val_loader_stage1, test_loader_stage1 = create_data_loaders(train_data_stage1, val_data_stage1, test_data_stage1, augmentation_prob=initial_augmentation_prob)\n",
    "\n",
    "        # 每个折叠重新初始化模型\n",
    "        model = MultiPathProteinClassifier()\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        model = model.to(device)\n",
    "\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.freeze_layers()\n",
    "        else:\n",
    "            model.freeze_layers()\n",
    "        \n",
    "        # 创建优化器\n",
    "        optimizer = get_optimizer(model)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.6, patience=2, min_lr=1e-6)\n",
    "\n",
    "        \n",
    "        # 第一阶段训练\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.set_stage(1)\n",
    "        else:\n",
    "            model.set_stage(1)\n",
    "\n",
    "        # 计算第一阶段的pos_weight\n",
    "        num_positive1 = train_data_stage1['label'].sum()\n",
    "        num_negative1 = len(train_data_stage1) - num_positive1\n",
    "        if num_positive1 < num_negative1:\n",
    "            pos_weight1 = torch.tensor([num_negative1 / num_positive1], dtype=torch.float32).to(device)\n",
    "        else:\n",
    "            pos_weight1 = torch.tensor([num_positive1 / num_negative1], dtype=torch.float32).to(device)\n",
    "\n",
    "        # 设置第一阶段的损失函数\n",
    "        loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight1)\n",
    "\n",
    "        print(\"Training Stage 1:\")\n",
    "        torch.cuda.empty_cache()  # 清理未使用的显存\n",
    "        model, best_model_state_fold = train_model(model, {'train': train_loader_stage1, 'val': val_loader_stage1}, optimizer, loss_function, scheduler, device, num_epochs, stage=1, fold=fold, initial_augmentation_prob=initial_augmentation_prob, patience=4)\n",
    "        \n",
    "        # 评估第一阶段\n",
    "        print(\"Evaluating Stage 1:\")\n",
    "        accuracy, precision, recall, f1, sensitivity, specificity, mcc, fold_labels, fold_probs, cm_stage1 = evaluate_model(model, test_loader_stage1, device, stage=1, fold=fold)\n",
    "        print(f\"Stage 1 Evaluation - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall (Sensitivity): {recall:.4f}, F1: {f1:.4f}, Specificity: {specificity:.4f}, MCC: {mcc:.4f}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_state = best_model_state_fold\n",
    "\n",
    "        del model, optimizer, scheduler, train_loader_stage1, val_loader_stage1, test_loader_stage1\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "# 执行第一阶段的分层k折交叉验证\n",
    "best_model_state_stage1 = stratified_k_fold_cross_validation_stage1(full_data, tokenizer, k=5, num_epochs=50, initial_augmentation_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb7e132-0eda-413a-bdde-d1b74ff6879e",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-11T07:40:26.126Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存第一阶段的最佳模型状态\n",
    "torch.save(best_model_state_stage1, \"Model/best_model_stage1_esm1b.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0e7ec-ca4f-4b92-babf-344a552a1740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44985998-f585-4ad1-ad99-66d129b97721",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-11T07:40:26.958Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model_weights(model, best_model_state_stage1):\n",
    "    \"\"\"\n",
    "    Load weights into the model, handling the differences between single GPU and multi-GPU training states.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model into which the weights will be loaded.\n",
    "        best_model_state_stage1 (dict): The state dictionary of the best model from stage 1.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If there is a mismatch between the keys in the state dictionary and the model.\n",
    "    \"\"\"\n",
    "    model_state_dict = model.state_dict()\n",
    "    new_state_dict = {}\n",
    "\n",
    "    # Iterate over the items in the state dictionary from stage 1\n",
    "    for k, v in best_model_state_stage1.items():\n",
    "        # Check if the current key in the model's state dictionary starts with 'module.'\n",
    "        if k.startswith(\"module.\") and not next(iter(model_state_dict.keys())).startswith(\"module.\"):\n",
    "            new_state_dict[k[7:]] = v\n",
    "        elif not k.startswith(\"module.\") and next(iter(model_state_dict.keys())).startswith(\"module.\"):\n",
    "            new_state_dict[\"module.\" + k] = v\n",
    "        else:\n",
    "            new_state_dict[k] = v\n",
    "\n",
    "    try:\n",
    "        # Load the newly constructed state dictionary into the model\n",
    "        model.load_state_dict(new_state_dict)\n",
    "        print(\"Model weights loaded successfully.\")\n",
    "    except KeyError as e:\n",
    "        print(\"Error in loading model weights: \", e)\n",
    "        raise\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df590fad-cc57-4106-9143-b61be7347bc8",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-11T07:40:27.875Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stratified_k_fold_cross_validation_stage2(second_stage_full_data, tokenizer, best_model_state_stage1, k=5, num_epochs=50, initial_augmentation_prob=0.5):\n",
    "     # 首先将数据集划分为训练+验证集和独立的测试集\n",
    "    train_val_data_stage2, test_data_stage2 = train_test_split(second_stage_full_data, test_size=0.2, random_state=42, stratify=second_stage_full_data['label'])\n",
    "    # 保存 test_data_stage2 到 CSV 文件\n",
    "    test_data_stage2.to_csv('Data/test_data_stage2.csv', index=False)\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    fold = 0\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for train_index, val_index in skf.split(train_val_data_stage2['sequence'], train_val_data_stage2['label']):\n",
    "        fold += 1\n",
    "        print(f\"Fold {fold}/{k}\")\n",
    "\n",
    "        # 使用StratifiedKFold的索引划分训练+验证集\n",
    "        train_data_stage2 = train_val_data_stage2.iloc[train_index]\n",
    "        val_data_stage2 = train_val_data_stage2.iloc[val_index]\n",
    "\n",
    "        # 创建数据加载器\n",
    "        train_loader_stage2, val_loader_stage2, test_loader_stage2 = create_data_loaders(train_data_stage2, val_data_stage2, test_data_stage2, augmentation_prob=initial_augmentation_prob)\n",
    "\n",
    "        # 每个折叠重新初始化模型\n",
    "        model = MultiPathProteinClassifier()\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            model = nn.DataParallel(model)\n",
    "        model = model.to(device)\n",
    "\n",
    "\n",
    "        # 加载第一阶段的最佳模型状态\n",
    "        model = load_model_weights(model, best_model_state_stage1)\n",
    "        \n",
    "       \n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.freeze_layers()\n",
    "        else:\n",
    "            model.freeze_layers()\n",
    "            \n",
    "        # 创建优化器\n",
    "        optimizer = get_optimizer(model)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.6, patience=2, min_lr=1e-6)\n",
    "\n",
    "        \n",
    "        # 第二阶段训练\n",
    "        if isinstance(model, nn.DataParallel):\n",
    "            model.module.set_stage(2)\n",
    "        else:\n",
    "            model.set_stage(2)\n",
    "\n",
    "        # 计算第二阶段的pos_weight\n",
    "        num_positive2 = train_data_stage2['label'].sum()\n",
    "        num_negative2 = len(train_data_stage2) - num_positive2\n",
    "        if num_positive2 < num_negative2:\n",
    "            pos_weight2 = torch.tensor([num_negative2 / num_positive2], dtype=torch.float32).to(device)\n",
    "        else:\n",
    "            pos_weight2 = torch.tensor([num_positive2 / num_negative2], dtype=torch.float32).to(device)\n",
    "\n",
    "        # 设置第二阶段的损失函数\n",
    "        loss_function = nn.BCEWithLogitsLoss(pos_weight=pos_weight2)\n",
    "\n",
    "        print(\"Training Stage 2:\")\n",
    "        torch.cuda.empty_cache()  # 清理未使用的显存\n",
    "        model, best_model_state_fold = train_model(model, {'train': train_loader_stage2, 'val': val_loader_stage2}, optimizer, loss_function, scheduler, device, num_epochs, stage=2, fold=fold, initial_augmentation_prob=initial_augmentation_prob, patience=4)\n",
    "\n",
    "        print(\"Evaluating Stage 2:\")\n",
    "        accuracy, precision, recall, f1, sensitivity, specificity, mcc, fold_labels, fold_probs, cm_stage2 = evaluate_model(model, test_loader_stage2, device, stage=2, fold=fold)\n",
    "        print(f\"Stage 2 Evaluation - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall (Sensitivity): {recall:.4f}, F1: {f1:.4f}, Specificity: {specificity:.4f}, MCC: {mcc:.4f}\")\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model_state = best_model_state_fold\n",
    "\n",
    "        del model, optimizer, scheduler, train_loader_stage2, val_loader_stage2, test_loader_stage2\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    return best_model_state\n",
    "\n",
    "# 执行第二阶段的分层k折交叉验证\n",
    "best_model_state_stage2 = stratified_k_fold_cross_validation_stage2(second_stage_full_data, tokenizer, best_model_state_stage1, k=5, num_epochs=50, initial_augmentation_prob=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeed0085-7a29-4d90-b302-a7ce91dd74e6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-11T07:40:28.798Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存第二阶段的最佳模型状态\n",
    "torch.save(best_model_state_stage2, \"Model/best_model_stage2_esm1b.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6955c6e3-2cec-4bae-96ae-a434c7552dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "mytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
